%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09�d�l
\documentclass[twocolumn]{jarticle} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  n�{ �o�[�W����
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm} 
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm} 
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm} 
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm} 
\setlength{\columnsep}{11mm}


%�y�߂������邲�Ƃ�(1.1)(1.2) �c(2.1)(2.2)�Ɛ����ԍ��������Ƃ��z
%\makeatletter
%\renewcommand{\theequation}{%
%\thesection.\arabic{equation}} %\@addtoreset{equation}{section}
%\makeatother

%\renewcommand{\arraystretch}{0.95} �sT�̐ݒ�

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[dvipdfmx]{graphicx}  %pLaTeX2e�d�l(�v\documentstyle ->\documentclass)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\twocolumn[
\noindent

\hspace{1em}
令和3年1月26日(火) 情報工学実験II発表資料
\hfill
B3 尾關 拓巳

\vspace{2mm}

\hrule

\begin{center}
{\Large \bf 深層強化学習を用いた株取引エージェントの戦略学習}
\end{center}


\hrule
\vspace{3mm}
]

\section{はじめに}
近年，機械学習の急速の発展に伴い，深層強化学習を用いた株取引が注目を集めている．
特に，深層強化学習の代表的な手法であるDeep $Q$-Network(DQN)や，それの拡張手法を6種類併用する手法であるRainbowを用いた株取引の研究\cite{d_mori}が報告されている．

そこで本実験では，まずOpenAI GymのCartPole問題をDeep $Q$-Networkで実装し,深層強化学習への理解を深める．次にDQNを用いた株取引エージェントの戦略学習を目的とする．

\section{株取引}
    東京証券取引所(東証)は日本最大の株式市場である．市場には東証一部，東証二部，マザーズ，JASDAQの4つの区分がある．株式を売買が行われる立会時間は午前9時から11時半の午前立会(前場)と午後12時半から15時の午後立会(後場)に分けられている．年末年始や休日祝日は休業日である．

    \subsection{四本値, 出来高}
    四本値とは設定時間内の始値，終値，高値，安値の情報を同等に示す指標である．始値は設定時間内で初めて取引された約定価格で，終値は最後に取引された約定価格である．高値は設定時間内に最も高く取引された約定価格，安値は最も安く取引された約定価格である．
    
    出来高とは設定期間内に取引が成立した数量のことを表す．

    \subsection{注文方法}
    株の注文方法には指値注文と成行注文の2通りがある．指値注文は自分で取引する値段を決定して注文する方法で，必ずしも注文が成立するとは限らない．成行注文は値段を指定しない注文方法である．買い注文であればそのとき出ている最も低い売り注文に対応し，売り注文であれば，そのとき出ている最も高い買い注文に対応するため，即座に注文が成立するという特徴がある．
    
    また，全国の証券取引上において株の注文量は100株単位と定められている．

\section{要素技術}

    \subsection{OpenAI Gym}
    OpenAIとは2015年に設立された人工知能を研究する非営利企業であり，Gym\cite{Gym}はその企業が作成した強化学習のシミュレーション用プラットフォームである．

        \subsubsection{CartPole-v0}
        CartPole-v0は，倒立振子を制御する問題である．状態はカートの位置，カートの速度，棒の角度，棒の角速度の4変数で表される．カートを左右に動かして棒を安定させ，倒れないようにする方法を学習することが強化学習のエージェントの目標となる．

    \subsection{Deep $Q$-Network}
    Deep $Q$-Network(DQN)は，強化学習の手法である$Q$-学習を用いた代表的な深層強化学習の手法である．DQNでは深層強化学習に基づく$Q$-Networkと呼ばれる，強化学習における価値に相当する$Q$値を多層ニューラルネットにより近似する．$Q$-Networkの更新には状態の遷移を経験として蓄積したものを利用するExprirnce Replayという工夫がなされている．

        \subsubsection{強化学習における報酬と価値}
        強化学習において報酬とは，現在の状態においてエージェントがある行動をとって次の状態に遷移したときに受ける信号である．良い結果を残す行動に正，悪い結果を残す行動に負の報酬を与え，途中の行動に対する報酬は0である．

        価値は，状態と行動の各ペアに対して，その良し悪しを数値化するために割り当てる．高い価値を持つということは，将来的に獲得する報酬の総和の期待値が高いことを意味する．$Q$学習やDeep $Q$-Networkにおける$Q$値は，この価値を表している．

        \subsubsection{Q-Network}
        $Q$-Networkとは，$Q$値を求める多層ニューラルネットで，入力層には状態変数を入力し，出力層は各行動ごとの$Q$値を出力する．連続値の状態空間を扱うことが可能で，一度の入力に基づくニューラルネットの出力計算により全種類の行動の$Q$値が得られるため，行動数によって計算量が増えることがほとんどないという利点をもつ．また未知の状態においても汎化された$Q$値で行動可能という利点を持つ．

        \subsubsection{Experience Replay}
        Experience Replayとは，過去の遷移情報を保存し，そこからランダムサンプリングすることで，データの時間的相関をなくす工夫である．この経験を蓄積したものをReplay Memoryと呼ぶ．
        
        状態$s_t$で行動$a_t$を選択したところ，報酬$r_t$を獲得し，次の状態が$s_{t+1}$であった場合，この情報($s_t$, $a_t$, $r_t$, $s_{t+1}$)を経験した順に記憶し続ける．設定したメモリの上限を超える場合は最も古い経験から破棄する．

        \subsubsection{Q-Networkの更新}
        十分にReplay memoryにデータを蓄えられたら，Replay memoryからランダムサンプリングし，以下の式に従って$Q$-Networkを更新する．

        \begin{equation}
            Q_\theta(s_t,a_t)\leftarrow(1-\alpha)Q_\theta(s_t,a_t)+\alpha(r+\gamma\max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1}))
        \end{equation}

        ここで$Q_\theta(s_t,a_t)$はパラメータ$\theta$を持つニューラルネットワークであり，$Q_\pi(s_t,a_t)$は教師信号出力用のニューラルネットで，$Q_\theta(s_t,a_t)$のコピーになっている．$\max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1})$は遷移先の状態$s_{t+1}$における最大の$Q$値，$\alpha$は学習率$(0\leq \alpha\leq 1)$, $r$は報酬，$\gamma$は割引率$(0\leq \gamma\leq 1)$, tは時刻である．

%\section{従来研究} 従来のもので実験するときはなくてもよい．

\section{提案手法}
本実験では，Deep $Q$-Networkを用いてCartPole問題を解き，さらに株式取引エージェントの戦略学習を目的とする．

    \subsection{CartPole問題}
        CartPole問題では，各ステップの状態を用いてカートを左右どちらに動かすかをDeep $Q$-Networkで学習し，倒立振子を制御させる．

        \subsubsection{$Q$-Network, Experience Replay}
        $Q$-Networkの入力層にはカートの位置$x$，カートの速度$v$，棒の角度$\theta$，棒の角速度$\omega$の4つの状態変数を入力し，出力層で左右に動かす行動の$Q$値を得る．常に$Q$値が大きい行動を選択すると，初めに与えるランダムな値の影響が大きくなるため，$\epsilon$-greedy法を用いて徐々にランダムな行動から最大$Q$値に従った行動を取るようする．また行動後はReplay Memoryに遷移情報を保存し，ランダムサンプリングする．後に$Q$-Networkの重みを学習，更新する．教師信号用の$Q_\pi(s_t,a_t)$は1試行が終わるたびに$Q_\theta(s_t,a_t)$と同じにする．

        \subsubsection{報酬}
        報酬は，各ステップで棒が立っていたら0，倒れたら-1，定めたステップ数以上立っていたら1を与える．

        \subsubsection{学習完了評価}
        学習完了の基準は各試行のstep数をある試行回数で平均し，それが定めた評価値以上であれば学習完了とする．
    
    \subsection{株取引}
    CartPole問題で実装したDeep $Q$-Networkを利用して株取引戦略を学習する．ある銘柄の一定期間における四本値と出来高のデータを用いて，DQNエージェントは東証の営業日の取引開始時に100株を始値で買うか買わないかの行動を学習する．ただしエージェントが株を取得した場合はその日の取引終了直前にその株を終値で売却するものとする．注文方法は指値注文で，注文は必ず通るものとする．

        \subsubsection{$Q$-Network, Experience Replay}
        $Q$-Networkの入力層には前日の四本値と出来高の5変数を与え，出力層は買うか買わないかの行動のQ値とする．
        
        Replay Memoryは取引終了時の行動が終わったのちに，前日の四本値と出来高，今日の行動，報酬，今日の四本値と出来高を保存する．

        \subsubsection{報酬設定}
        ある期間内の学習を1試行と定義し，その期間で損益がプラスであれば報酬を1与える．1日ごとの行動では，買って損した場合と買わなかったが株価が上昇した場合は-1を与え，それ以外は0とする． また，試行ごとに異なる期間を学習させて時間的な汎用性を高める．

\section{数値実験}
        \subsection{CartPole問題}
        表1にCartPole問題における実験設定，表2,3に$Q$-Networkの設定および構造を示す．

        \begin{table}
            \caption{CartPole問題実験設定}
            \begin{tabular}{|c|c|} \hline
                最大試行回数 & 200回 \\ \hline
                1試行のstep数 & 200回 \\ \hline
                学習完了基準となる評価値 & 195 \\ \hline
                報酬を与える基準ステップ数 & 195 \\ \hline
                学習完了評価の平均を計算する試行回数 & 10 \\ \hline
                割引率 & 0.99 \\ \hline
                Experience Replayのメモリ上限 & 10000 \\ \hline
            \end{tabular}
        \end{table}

        \begin{table}
            \centering
            \caption{$Q$-Networkの設定}
            \begin{tabular}{|c|c|} \hline
                最適化アルゴリズム & Adam \\ \hline
                損失関数 & Huber関数($\delta=1.0$)\\ \hline %lossがδより大きいと平均絶対誤差，lossがδより小さいと平均二乗誤差，今回はδ＝1
                学習率 & 0.0004 \\ \hline
                %$Q$-Networkを更新するバッチの大きさ & 32 \\ \hline
            \end{tabular}
        \end{table}

        \begin{table}
            \caption{$Q$-Networkの構造}
            \begin{tabular}{|c|c|c|c|c|} \hline
                層 & 入力層 & 隠れ層 & 隠れ層 & 出力層 \\ \hline
                ニューロン数 & 4 & 16 & 16 & 2 \\ \hline
            \end{tabular}
        \end{table}
        
        \subsection{エージェントの株取引戦略学習}
        扱う銘柄は東証一部に上場している任天堂で，2016年から2020年の5年間における四本値と出来高を取得した．また，1試行は10営業日とした．
        
        $Q$-Networkや教師信号用の$Q_\pi(s_t,a_t)$の実験設定は，ネットワークの入力層を5，学習率を0.0005にし，それ以外はCartPole問題のときと同じで実験した．
    
\section{結果と考察}
        \subsection{CartPole問題}
        図1,2に結果として得られた試行ごとのstep数の推移の例を示す．縦軸は棒を立て続けたstep数で，横軸は試行回数である．

        \begin{figure}
            \includegraphics[width=9cm]{step_episode_1.png}
            \caption{試行ごとのstep数の推移1}
        \end{figure}

        \begin{figure}
            \includegraphics[width=9cm]{step_episode_2.png}
            \caption{試行ごとのstep数の推移2}
        \end{figure}

        どちらも徐々にステップ数が増加し，最後から数試行では200step連続で倒立振子を制御することができていることがわかる．予備実験により，100試行あればほぼ上記結果と同じように学習完了することがわかった．

        \subsection{エージェントの学習戦略}
        図3に結果として得られた直近200日の取引の損益の推移の例，図4に全営業日の取引の損益の推移の例を示す．縦軸は獲得した損益の合計で，横軸は営業日である．

        \begin{figure}
            \includegraphics[width=9cm]{recentdays_profit_transition_1.png}
            \caption{直近200日の損益の推移1}
        \end{figure}

        \begin{figure}
            \includegraphics[width=9cm]{all_profit_transition_0.png}
            \caption{全営業日の取引の損益の推移}
        \end{figure}

        エージェントによる学習が進んでいると考えられる直近200日の損益は増加傾向にあった．しかし図4では400日から800日頃にかけては損益が大きく下がっており，安定した成果を出すことはできていない．図5に全営業日の任天堂の株価を示す．縦軸は任天堂の株価で，横軸は営業日である．

        \begin{figure}
            \includegraphics[width=9cm]{kabuka.png}
            \caption{全営業日の任天堂の株価}
        \end{figure}

        任天堂の株価に着目すると，400日から500日頃は上昇傾向で，500日から800日頃は下降傾向である．このことから，株価が上昇傾向から下降傾向に変化しても，エージェントは株価の上昇トレンドが継続していると判断していると考えられる．

        また，実験中の5年間でbuy \& holdの場合は約500万円の利益を得ており，今回の実験でエージェントが得た利益よりも大きかった．
        
\section{おわりに}
本実験では，Deep $Q$-Networkを用いてCartPole問題を解き，株取引戦略の学習をした．CartPole問題は順調に学習が進み，深層強化学習への理解を深めることができた．エージェントの戦略学習については安定した利益を獲得する戦略を学習することはできなかった．

今後の課題としては，より発展的な深層強化学習の理解，株取引における学習期間，1試行あたりに学習する営業日，報酬の与え方などのの調整，さらに板情報や気配値を考慮したより現実的な取引戦略の学習などがあげられる．

%参考文献
\bibliographystyle{unsrt}
\bibliography{sankou}

% \begin{thebibliography}{9}
%     \bibitem{d_mori} 森 大典. 深層強化学習Rainbowを用いたデイトレード戦略の構築. 2018.
%     \bibitem{OpenAI} 
% \end{thebibliography}

\end{document}


