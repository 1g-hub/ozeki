%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09�d�l
\documentclass[twocolumn]{jarticle} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  n�{ �o�[�W����
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm} 
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm} 
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm} 
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm} 
\setlength{\columnsep}{11mm}


%�y�߂������邲�Ƃ�(1.1)(1.2) �c(2.1)(2.2)�Ɛ����ԍ��������Ƃ��z
%\makeatletter
%\renewcommand{\theequation}{%
%\thesection.\arabic{equation}} %\@addtoreset{equation}{section}
%\makeatother

%\renewcommand{\arraystretch}{0.95} �sT�̐ݒ�

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[dvipdfmx]{graphicx}  %pLaTeX2e�d�l(�v\documentstyle ->\documentclass)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\twocolumn[
\noindent

\hspace{1em}
令和3年 1月 26日(火曜日) 情報工学実験II発表資料
\hfill
B3 尾關 拓巳

\vspace{2mm}

\hrule

\begin{center}
{\Large \bf 深層強化学習を用いた株取引エージェントの戦略学習}
\end{center}


\hrule
\vspace{3mm}
]

\section{はじめに}
近年，機械学習の急速の発展に伴い，深層強化学習を用いた株取引が注目を集めている．
特に，深層Q学習(Deep $Q$-Network: DQN)の基本的な拡張手法を6種類組み合わせた手法であるRainbowを用いた株取引の研究\cite{d_mori}もなされている．

そこで本実験では，Open AI GymのCartPole問題をDQNで実装し深層強化学習への理解を深める．次にDQNを用いた株取引エージェントの戦略学習を行うことを目的とする．

\section{株取引}
    東京証券取引所（東証）は日本最大の株式市場である．市場には東証一部，東証二部，マザーズ，JASDAQの４つの区分がある．株式を売買が行われる立会時間は午前9時から11時半の午前立会(前場)と午後12時半から15時の午後立会(後場)に分けられている．年末年始や休日祝日は休業日である．
    \subsection{四本値, 出来高}

    四本値とは設定時間内の始値，終値，高値，安値のことである．始値は設定時間内で初めて取引された株価で，終値は最後に取引された株価である．高値は設定時間内に最も高く取引された株価，安値は最も安く取引された株価である．
    
    出来高とは設定期間内に取引が成立した数量のことである．
    \subsection{注文方法}
    株の注文方法には指値注文と成行注文の2通りがある．指値注文は自分で取引する値段を決定して注文を行う方法で，必ずしも注文が成立するとは限らない．成行注文は値段を指定しない注文方法である．買い注文であればそのとき出ている最も低い売り注文に対応し，売り注文であれば，そのとき出ている最も高い買い注文に対応するため，即座に注文が成立するという特徴がある．
    
    また，全国の証券取引上において株の注文量は100株単位と定められている．
\section{要素技術}
    \subsection{Open AI Gym}
    Open AIとは2015年に設立された人工知能を研究する非営利企業であり，Gymはその企業が作成した強化学習のシミュレーション用プラットフォームである．
        \subsubsection{CartPole-v0}
        CartPole-v0は，倒立振子を制御する問題である．カートには摩擦がなく，状態はカートの位置，カートの速度，棒の角度，棒の角速度の4変数で表される．

    \subsection{Deep $Q$-Network}
    Deep $Q$-Network(DQN)は，代表的な強化学習手法である$Q$-Learningを用いた深層強化学習である．DQNでは深層強化学習に基づく$Q$-Networkと呼ばれる，強化学習における価値に相当する$Q$値を多層ニューラルネットにより近似する．$Q$-Networkの更新にはreplay memoryと呼ばれる状態の遷移を経験として蓄積したものを利用する．

        \subsubsection{Q-Network}
        $Q$-Networkとは，$Q$値を求める多層ニューラルネットで，状態を入力とし，出力層においては各行動ごとの$Q$値を持つ．一度の入力に基づくニューラルネットの出力計算により，全種類の行動の$Q$値が得られるため，行動数によって計算量が増えることがほとんどないという利点を持つ．

        \subsubsection{Experience Replay}
        Experience Replayとは，過去の遷移情報を保存し，そこからランダムサンプリングすることで，擬似的にデータの時間的偏りをなくす工夫である．この経験の蓄積をReplay Memoryと呼ぶ．遷移情報は「状態$s_t$で行動$a_t$を選択したところ，報酬$r_t$を獲得し，次の状態が$s_{t+1}$であった」場合，これらを含む4つの組から構成される($s_t$, $a_t$, $r_t$, $s_{t+1}$)を経験した順に記憶し続ける．設定したメモリの上限を超える場合は最も古い経験から破棄する．

        \subsubsection{Q-Networkの更新}
        十分にreplay memoryにデータを蓄えられたら，replay memoryからランダムサンプリングし，以下の式に従って$Q$-Networkを更新する．
        \begin{equation}
            Q_\theta(s_t,a_t)\leftarrow(1-\alpha)Q_\theta(s_t,a_t)+\alpha(r+\gamma\max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1}))
        \end{equation}
        ここで$Q_\theta(s_t,a_t)$はパラメータ$\theta$を持つニューラルネットワークであり，$Q_\pi(s_t,a_t)$は教師信号出力用のニューラルネットで，$Q_\theta(s_t,a_t)$のコピーになっている．$\max_{a_{t+1}}Q_\pi(s_{t+1},a_{t+1})$は遷移先の状態$s_{t+1}$における最大の$Q$値，$\alpha$は学習率$(0\leq \alpha\leq 1)$, $r$は報酬，$\gamma$は割引率$(0\leq \gamma\leq 1)$, tは時刻である．

%\section{従来研究} 従来のもので実験するときはなくてもよい．

\section{提案手法}
本実験では，DQNを用いてCartPole問題を解き，さらに株式取引エージェントの戦略学習を行うことを目的とする．
    \subsection{CartPole問題}
        CartPole問題では，各ステップの状態を用いてカートを左右どちらに動かすかをDQNで学習し，倒立振子を制御させる．
        \subsubsection{$Q$-Network, Experience Replay}
        $Q$-Networkの入力層にはカートの位置$x$，カートの速度$v$，棒の角度$\theta$，棒の角速度$\omega$の４つの状態変数を入力し，出力層で左右に動かす行動の$Q$値を得る．常に$Q$値が大きい行動を選択すると，初めに与えるランダムな値の影響が大きくなるため，$\epsilon$-greedy法を用いて徐々にランダムな行動から$Q$値に従った行動を取るようする．行動をとったあとはReplay Memoryに遷移情報を保存し，ランダムサンプリングを行う．後に$Q$-Networkの重みを学習，更新する．教師信号用の$Q_\pi(s_t,a_t)$は1試行が終わるたびに$Q_\theta(s_t,a_t)$と同じにする．
        \subsubsection{報酬}
        報酬は，各ステップで棒が立っていたら0，倒れたら-1，定めたステップ数以上立っていたら1を与える．
        \subsubsection{学習完了評価}
        学習完了の基準は各試行のstep数をある試行回数で平均し，それが定めた評価値以上であれば学習完了とする．
    
    \subsection{株取引}
    CartPole問題で実装したDQNを利用して株取引戦略を学習する．ある銘柄の一定期間における四本値と出来高のデータを用いて，DQNエージェントは東証の営業日の取引開始時に100株を始値で買うか買わないかの行動を学習する．ただしエージェントが株を取得した場合はその日の取引終了直前にその株を終値で売却するものとする．注文方法は指値注文で，注文は必ず通るものとする．
        \subsubsection{$Q$-Network, Experience Replay}
        $Q$-Networkの入力層には前日の四本値と出来高の5変数を与え，出力層は買うか買わないかの行動のQ値とする．
        
        Replay Memoryは取引終了時の行動が終わったのちに，前日の四本値と出来高，今日の行動，報酬，今日の四本値と出来高を保存する．

        \subsubsection{報酬設定}
        ある期間内の学習を1試行と定義し，その期間で損益がプラスであれば報酬を1与える．1日ごとの行動では，買って損した場合と買わなかったが株価が上昇した場合は-1を与え，それ以外は0とする． また，試行ごとに異なる期間を学習させて時間的な汎用性を高める．
\section{数値実験}
        \subsection{CartPole問題}
        表1にCartPole問題における実験設定，表2,3に$Q$-Networkおよび$Q_\pi(s_t,a_t)$のネットワークの設定，構造を示す．
        \begin{table}
            \caption{CartPole問題実験設定}
            \begin{tabular}{|c|c|} \hline
                最大試行回数 & 200回 \\ \hline
                1試行のstep数 & 200回 \\ \hline
                学習完了基準となる評価値 & 195 \\ \hline
                報酬を与える基準ステップ数 & 195 \\ \hline
                学習完了評価の平均計算を行う試行回数 & 10 \\ \hline
                割引率 & 0.99 \\ \hline
                Experience Replayのメモリ上限 & 10000 \\ \hline
            \end{tabular}
        \end{table}
        \begin{table}
            \centering
            \caption{$Q$-Network，$Q_\pi(s_t,a_t)$のネットワークの設定}
            \begin{tabular}{|c|c|} \hline
                最適化アルゴリズム & Adam \\ \hline
                損失関数 & Huber関数($\delta=1.0$)\\ \hline %lossがδより大きいと平均絶対誤差，lossがδより小さいと平均二乗誤差，今回はδ＝１
                学習率 & 0.0004 \\ \hline
                %$Q$-Networkを更新するバッチの大きさ & 32 \\ \hline
            \end{tabular}
        \end{table}
        \begin{table}
            \caption{$Q$-Network，$Q_\pi(s_t,a_t)$のネットワークの構造}
            \begin{tabular}{|c|c|c|c|c|} \hline
                層 & 入力層 & 隠れ層 & 隠れ層 & 出力層 \\ \hline
                ニューロン数 & 4 & 16 & 16 & 2 \\ \hline
            \end{tabular}
        \end{table}
        
        \subsection{エージェントの戦略学習}
        扱う銘柄は東証一部に上場している任天堂で，2016年から2020年の5年間における四本値と出来高を取得した．また，1試行は10営業日とした．
        
        $Q$-Networkや教師信号用の$Q_\pi(s_t,a_t)$の実験設定は，ネットワークの入力層を5，学習率を0.0005にし，それ以外はCartPole問題のときと同じで実験を行った．
    
\section{結果と考察}
        \subsection{CartPole問題}
        図1,2に結果として得られた試行ごとのstep数の推移の例を示す．
        \begin{figure}
            \includegraphics[width=9cm]{step_episode_1.png}
            \caption{試行ごとのstep数の推移1}
        \end{figure}
        \begin{figure}
            \includegraphics[width=9cm]{step_episode_2.png}
            \caption{試行ごとのstep数の推移2}
        \end{figure}
        どちらも徐々にステップ数が増加し，最後から数試行では200step連続で倒立振子を制御することができていることがわかる．何度か実験を行った結果，おおよそ100試行あればこれらと同じように学習完了することがわかった．

        \subsection{エージェントの学習戦略}
        図3, 4に結果として得られた直近200日の取引の損益の推移の例，図5にすべての営業日の取引の損益の推移の例を示す．
        \begin{figure}
            \includegraphics[width=9cm]{recentdays_profit_transition_1.png}
            \caption{直近200日の損益の推移1}
        \end{figure}
        \begin{figure}
            \includegraphics[width=9cm]{recentdays_profit_transition_2.png}
            \caption{直近200日の損益の推移2}
        \end{figure}
        \begin{figure}
            \includegraphics[width=9cm]{all_profit_transition.png}
            \caption{すべての営業日の取引の損益の推移}
        \end{figure}
        エージェントによる学習が進んでいると考えられる直近２００日の損益はどの図を見ても増加傾向にあった．しかし図5の400日目から800日目にかけては損益が大きく下がっており，安定した成果を出すことはできていない．
        
\section{おわりに}
本実験では，DQNを用いてCartPole問題を解き，株取引戦略を学習した．CartPole問題は順調に学習が進み，実装を通して深層強化学習への理解を深めることができた．エージェントの戦略学習については安定した利益をうむ戦略を学習することはできなかった．

今後の課題としては，より発展的な深層強化学習の理解や，株取引における学習期間，1試行あたりに学習する営業日，報酬の与え方の調整などが挙げられる．

%参考文献
\begin{thebibliography}{9}
    \bibitem{d_mori} 森 大典. 深層強化学習Rainbowを用いたデイトレード戦略の構築. 2018.
\end{thebibliography}

\end{document}


